{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Bot Training with Stockfish Evaluation on Google Colab\n",
    "\n",
    "This notebook trains a deep reinforcement learning chess agent using Stockfish evaluation to provide a richer reward signal. This approach significantly improves training efficiency compared to the standard reward function based only on game outcomes and material advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, we'll install the required dependencies including python-chess, PyTorch, and Stockfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install python-chess==1.11.2 torch numpy matplotlib\n",
    "!apt-get update && apt-get install -y stockfish\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p /content/models\n",
    "!mkdir -p /content/data/synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import chess.engine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture\n",
    "\n",
    "We'll define the DQN architecture with a mask layer to handle valid moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Layer for handling valid moves\n",
    "class MaskLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Ensure mask is boolean or float tensor of 0s and 1s\n",
    "        # Reshape mask to match the output shape if necessary\n",
    "        mask_reshaped = mask.view_as(x)\n",
    "        # Apply mask: set invalid move scores to a very small number (or -inf)\n",
    "        # Using -inf ensures that softmax output for invalid moves is zero\n",
    "        masked_output = x.masked_fill(mask_reshaped == 0, -float(\"inf\"))\n",
    "        return masked_output\n",
    "\n",
    "# Deep Q-Network (DQN) Architecture\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Input: 8x8 board, 16 channels (6 white, 6 black, empty, castling, en passant, player)\n",
    "        self.conv1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Flattened size: 128 * 8 * 8 = 8192\n",
    "        self.fc1 = nn.Linear(128 * 64, 4096) # Reduced intermediate layer size\n",
    "        self.fc2 = nn.Linear(4096, 4096) # Output: 64*64 = 4096 possible moves\n",
    "        \n",
    "        self.mask_layer = MaskLayer()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1) # Flatten all dimensions except batch\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # Raw scores for each move\n",
    "        \n",
    "        if mask is not None:\n",
    "            x = self.mask_layer(x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions help convert chess boards to tensors and create move masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert chess board to tensor representation\n",
    "def board_to_tensor(board):\n",
    "    \"\"\"Converts a chess.Board object to a 16x8x8 tensor.\"\"\"\n",
    "    tensor = np.zeros((16, 8, 8), dtype=np.float32)\n",
    "    \n",
    "    pieces = [chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING]\n",
    "    \n",
    "    for i, piece in enumerate(pieces):\n",
    "        # White pieces\n",
    "        for square in board.pieces(piece, chess.WHITE):\n",
    "            rank, file = chess.square_rank(square), chess.square_file(square)\n",
    "            tensor[i, rank, file] = 1\n",
    "        # Black pieces\n",
    "        for square in board.pieces(piece, chess.BLACK):\n",
    "            rank, file = chess.square_rank(square), chess.square_file(square)\n",
    "            tensor[i + 6, rank, file] = 1\n",
    "            \n",
    "    # Castling rights (binary encoded)\n",
    "    if board.has_kingside_castling_rights(chess.WHITE): tensor[12, 0, 7] = 1\n",
    "    if board.has_queenside_castling_rights(chess.WHITE): tensor[12, 0, 0] = 1\n",
    "    if board.has_kingside_castling_rights(chess.BLACK): tensor[13, 7, 7] = 1\n",
    "    if board.has_queenside_castling_rights(chess.BLACK): tensor[13, 7, 0] = 1\n",
    "\n",
    "    # En passant square\n",
    "    if board.ep_square:\n",
    "        rank, file = chess.square_rank(board.ep_square), chess.square_file(board.ep_square)\n",
    "        tensor[14, rank, file] = 1\n",
    "        \n",
    "    # Player to move (1 for White, 0 for Black - consistent layer)\n",
    "    if board.turn == chess.WHITE:\n",
    "        tensor[15, :, :] = 1\n",
    "    else:\n",
    "        tensor[15, :, :] = 0 # Or -1 if preferred\n",
    "        \n",
    "    return torch.from_numpy(tensor).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "# Helper function to create the move mask\n",
    "def create_move_mask(board):\n",
    "    \"\"\"Creates a 4096-element mask tensor for legal moves.\"\"\"\n",
    "    mask = torch.zeros(4096, dtype=torch.float32)\n",
    "    for move in board.legal_moves:\n",
    "        index = move.from_square * 64 + move.to_square\n",
    "        # Handle promotion - for simplicity, allow any promotion for now\n",
    "        if move.promotion:\n",
    "             # Simple approach: mark the basic move index\n",
    "             mask[index] = 1\n",
    "        else:\n",
    "            mask[index] = 1\n",
    "    return mask # No batch dimension here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Experience Replay Memory\n",
    "\n",
    "This helps store and sample experiences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Experience Replay memory\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done', 'mask', 'next_mask'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save an experience\"\"\"\n",
    "        self.memory.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Trainer with Stockfish Evaluation\n",
    "\n",
    "This is the main class that handles training with Stockfish evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessTrainerWithStockfish:\n",
    "    def __init__(self, model_dir=\"/content/models\", stockfish_path=\"stockfish\"):\n",
    "        # Create model directory if it doesn't exist\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        \n",
    "        self.model_dir = model_dir\n",
    "        \n",
    "        # Check for GPU availability\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize Stockfish engine\n",
    "        try:\n",
    "            self.stockfish = chess.engine.SimpleEngine.popen_uci(stockfish_path)\n",
    "            print(f\"Stockfish engine initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Stockfish engine: {e}\")\n",
    "            print(\"Falling back to material-based evaluation\")\n",
    "            self.stockfish = None\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network is only used for inference\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
    "        \n",
    "        # Initialize replay memory\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 1000\n",
    "        self.target_update = 10  # Update target network every N episodes\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state, mask, board):\n",
    "        \"\"\"Select an action using epsilon-greedy policy\"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                        np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        state = state.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "        \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # Use policy network to select best action\n",
    "                q_values = self.policy_net(state, mask)\n",
    "                action_idx = q_values.max(1)[1].item()\n",
    "                \n",
    "                # Convert action index to chess move\n",
    "                from_square = action_idx // 64\n",
    "                to_square = action_idx % 64\n",
    "                \n",
    "                # Check if this is a legal move\n",
    "                move = chess.Move(from_square, to_square)\n",
    "                \n",
    "                # Handle promotion\n",
    "                piece = board.piece_at(from_square)\n",
    "                if piece and piece.piece_type == chess.PAWN:\n",
    "                    if board.turn == chess.WHITE and chess.square_rank(to_square) == 7:\n",
    "                        move.promotion = chess.QUEEN\n",
    "                    elif board.turn == chess.BLACK and chess.square_rank(to_square) == 0:\n",
    "                        move.promotion = chess.QUEEN\n",
    "                \n",
    "                # If move is not legal, choose a random legal move\n",
    "                if move not in board.legal_moves:\n",
    "                    legal_moves = list(board.legal_moves)\n",
    "                    if not legal_moves: return None, None # No legal moves\n",
    "                    move = random.choice(legal_moves)\n",
    "                    action_idx = move.from_square * 64 + move.to_square\n",
    "        else:\n",
    "            # Choose a random legal move\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            if not legal_moves: return None, None # No legal moves\n",
    "            move = random.choice(legal_moves)\n",
    "            action_idx = move.from_square * 64 + move.to_square\n",
    "            \n",
    "        return action_idx, move\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        \"\"\"Perform one step of optimization\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample a batch from memory\n",
    "        experiences = self.memory.sample(self.batch_size)\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.long, device=self.device).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=self.device)\n",
    "        mask_batch = torch.cat(batch.mask).to(self.device)\n",
    "        \n",
    "        # Handle non-final states\n",
    "        non_final_mask_indices = torch.tensor(tuple(map(lambda d: not d, batch.done)), dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        non_final_next_states = torch.cat([s for s, d in zip(batch.next_state, batch.done) if not d]).to(self.device)\n",
    "        non_final_next_masks = torch.cat([m for m, d in zip(batch.next_mask, batch.done) if not d]).to(self.device)\n",
    "        \n",
    "        # Compute Q(s_t, a)\n",
    "        state_action_values = self.policy_net(state_batch, mask_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        if non_final_next_states.size(0) > 0: # Check if there are any non-final states\n",
    "             with torch.no_grad():\n",
    "                 next_state_values[non_final_mask_indices] = self.target_net(non_final_next_states, non_final_next_masks).max(1)[0]\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "        \n",
    "        # Compute Huber loss (Smooth L1 Loss)\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to stabilize training\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def calculate_stockfish_reward(self, board, prev_board=None):\n",
    "        \"\"\"Calculate reward based on Stockfish evaluation\"\"\"\n",
    "        if self.stockfish is None:\n",
    "            # Fallback to material advantage if Stockfish is not available\n",
    "            return self.calculate_reward(board)\n",
    "        \n",
    "        try:\n",
    "            # Get current board evaluation\n",
    "            current_score = self.stockfish.analyse(board=board, limit=chess.engine.Limit(depth=5))['score'].relative.score(mate_score=10000)\n",
    "            \n",
    "            # If we have a previous board, calculate the difference in evaluation\n",
    "            if prev_board is not None:\n",
    "                prev_score = self.stockfish.analyse(board=prev_board, limit=chess.engine.Limit(depth=5))['score'].relative.score(mate_score=10000)\n",
    "                # Convert centipawn to pawn score and subtract a small penalty for each move\n",
    "                reward = (current_score - prev_score) / 100.0 - 0.01\n",
    "            else:\n",
    "                # If no previous board, just use the current evaluation\n",
    "                reward = current_score / 100.0\n",
    "                \n",
    "            # Check for terminal states\n",
    "            if board.is_checkmate():\n",
    "                reward = 100.0 if board.turn == chess.BLACK else -100.0  # Positive reward if white wins, negative if black wins\n",
    "            elif board.is_stalemate() or board.is_insufficient_material():\n",
    "                reward = 0.0  # Neutral reward for draw\n",
    "                \n",
    "            return reward\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating Stockfish reward: {e}\")\n",
    "            # Fallback to material advantage\n",
    "            return self.calculate_reward(board)\n",
    "    \n",
    "    def calculate_reward(self, board):\n",
    "        \"\"\"Calculate reward based on the board state (fallback method)\"\"\"\n",
    "        # Basic reward function\n",
    "        if board.is_checkmate():\n",
    "            # High reward/penalty for checkmate\n",
    "            return 1.0 if board.turn == chess.BLACK else -1.0\n",
    "        elif board.is_stalemate() or board.is_insufficient_material():\n",
    "            # Small penalty for draw\n",
    "            return -0.1\n",
    "        \n",
    "        # Material advantage reward\n",
    "        material_advantage = self.calculate_material_advantage(board)\n",
    "        \n",
    "        # Position evaluation reward\n",
    "        position_score = self.evaluate_position(board)\n",
    "        \n",
    "        # Combine rewards\n",
    "        return 0.01 * material_advantage + 0.005 * position_score\n",
    "    \n",
    "    def calculate_material_advantage(self, board):\n",
    "        \"\"\"Calculate material advantage for the current player\"\"\"\n",
    "        piece_values = {\n",
    "            chess.PAWN: 1,\n",
    "            chess.KNIGHT: 3,\n",
    "            chess.BISHOP: 3,\n",
    "            chess.ROOK: 5,\n",
    "            chess.QUEEN: 9,\n",
    "            chess.KING: 0  # King's value doesn't contribute to material advantage\n",
    "        }\n",
    "        \n",
    "        white_material = sum(len(board.pieces(piece_type, chess.WHITE)) * value \n",
    "                            for piece_type, value in piece_values.items())\n",
    "        black_material = sum(len(board.pieces(piece_type, chess.BLACK)) * value \n",
    "                            for piece_type, value in piece_values.items())\n",
    "        \n",
    "        # Return advantage from perspective of current player\n",
    "        return white_material - black_material if board.turn == chess.WHITE else black_material - white_material\n",
    "    \n",
    "    def evaluate_position(self, board):\n",
    "        \"\"\"Simple position evaluation\"\"\"\n",
    "        # Center control\n",
    "        center_squares = [chess.E4, chess.D4, chess.E5, chess.D5]\n",
    "        center_control = sum(1 if board.piece_at(sq) is not None and board.piece_at(sq).color == board.turn else 0 \n",
    "                            for sq in center_squares)\n",
    "        \n",
    "        # Mobility (number of legal moves)\n",
    "        mobility = len(list(board.legal_moves))\n",
    "        \n",
    "        # Combine factors\n",
    "        return center_control + 0.1 * mobility\n",
    "    \n",
    "    def train_self_play(self, num_episodes=1000):\n",
    "        \"\"\"Train the model through self-play\"\"\"\n",
    "        print(f\"Starting self-play training with Stockfish evaluation for {num_episodes} episodes...\")\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Initialize the environment\n",
    "            board = chess.Board()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            # Get initial state\n",
    "            state = board_to_tensor(board)\n",
    "            mask = create_move_mask(board).unsqueeze(0)\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                # Store the current board for reward calculation\n",
    "                prev_board = board.copy()\n",
    "                \n",
    "                # Select and perform an action\n",
    "                action_idx, move = self.select_action(state, mask, board)\n",
    "                \n",
    "                if move is None: # No legal moves\n",
    "                    break \n",
    "                    \n",
    "                # Execute the move\n",
    "                board.push(move)\n",
    "                \n",
    "                # Get the next state\n",
    "                next_state = board_to_tensor(board)\n",
    "                next_mask = create_move_mask(board).unsqueeze(0)\n",
    "                \n",
    "                # Calculate reward using Stockfish\n",
    "                reward = self.calculate_stockfish_reward(board, prev_board)\n",
    "                \n",
    "                # Check if the game is over\n",
    "                done = board.is_game_over()\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action_idx, next_state, reward, done, mask, next_mask)\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                mask = next_mask\n",
    "                \n",
    "                # Perform one step of optimization\n",
    "                loss = self.optimize_model()\n",
    "                if loss is not None:\n",
    "                    losses.append(loss)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Limit episode length to avoid very long games\n",
    "                if episode_length >= 200:\n",
    "                    done = True\n",
    "            \n",
    "            # Update the target network\n",
    "            if episode % self.target_update == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:])\n",
    "                avg_length = np.mean(episode_lengths[-10:])\n",
    "                avg_loss = np.mean(losses[-100:]) if losses else 0 # Avg loss over last 100 steps\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Episode {episode+1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Avg Length: {avg_length:.1f} | Avg Loss: {avg_loss:.4f} | Steps: {self.steps_done} | Time: {elapsed_time:.1f}s\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                self.save_model(f\"model_stockfish_ep{episode+1}.pt\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model(\"model_stockfish_final.pt\")\n",
    "        print(\"Self-play training completed!\")\n",
    "        \n",
    "        # Close Stockfish engine\n",
    "        if self.stockfish:\n",
    "            self.stockfish.quit()\n",
    "        \n",
    "        return episode_rewards, episode_lengths, losses\n",
    "    \n",
    "    def train_from_pgn(self, pgn_file, num_games=1000):\n",
    "        \"\"\"Train the model from PGN games with Stockfish evaluation\"\"\"\n",
    "        print(f\"Training from PGN file with Stockfish evaluation: {pgn_file}\")\n",
    "        \n",
    "        if not os.path.exists(pgn_file):\n",
    "            print(f\"Error: PGN file not found at {pgn_file}\")\n",
    "            return []\n",
    "            \n",
    "        # Open PGN file\n",
    "        with open(pgn_file) as f:\n",
    "            game_count = 0\n",
    "            losses = []\n",
    "            processed_moves = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            while game_count < num_games:\n",
    "                # Read the next game\n",
    "                try:\n",
    "                    game = chess.pgn.read_game(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading game: {e}. Skipping.\")\n",
    "                    continue\n",
    "                    \n",
    "                if game is None:\n",
    "                    print(\"Reached end of PGN file.\")\n",
    "                    break  # End of file\n",
    "                \n",
    "                # Process the game\n",
    "                board = game.board()\n",
    "                moves = list(game.mainline_moves())\n",
    "                \n",
    "                # Skip very short games\n",
    "                if len(moves) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Process each position in the game\n",
    "                prev_board = None\n",
    "                for i, move in enumerate(moves):\n",
    "                    # Get current state\n",
    "                    state = board_to_tensor(board)\n",
    "                    mask = create_move_mask(board).unsqueeze(0)\n",
    "                    \n",
    "                    # Store current board for reward calculation\n",
    "                    prev_board = board.copy()\n",
    "                    \n",
    "                    # Convert move to action index\n",
    "                    action_idx = move.from_square * 64 + move.to_square\n",
    "                    \n",
    "                    # Make the move\n",
    "                    board.push(move)\n",
    "                    \n",
    "                    # Get next state\n",
    "                    next_state = board_to_tensor(board)\n",
    "                    next_mask = create_move_mask(board).unsqueeze(0)\n",
    "                    \n",
    "                    # Calculate reward using Stockfish\n",
    "                    reward = self.calculate_stockfish_reward(board, prev_board)\n",
    "                    \n",
    "                    # Check if game is over\n",
    "                    done = board.is_game_over()\n",
    "                    \n",
    "                    # Store transition in memory\n",
    "                    self.memory.push(state, action_idx, next_state, reward, done, mask, next_mask)\n",
    "                    processed_moves += 1\n",
    "                    \n",
    "                    # Perform optimization step if enough samples\n",
    "                    if len(self.memory) >= self.batch_size:\n",
    "                        loss = self.optimize_model()\n",
    "                        if loss is not None:\n",
    "                            losses.append(loss)\n",
    "                            \n",
    "                        # Update target network periodically based on steps/batches\n",
    "                        if processed_moves % (self.target_update * self.batch_size) == 0: \n",
    "                            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "                game_count += 1\n",
    "                \n",
    "                # Print progress\n",
    "                if game_count % 10 == 0:\n",
    "                    avg_loss = np.mean(losses[-1000:]) if losses else 0 # Avg loss over last 1000 steps\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(f\"Processed {game_count}/{num_games} games | Moves: {processed_moves} | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "                \n",
    "                # Save model periodically\n",
    "                if game_count % 100 == 0:\n",
    "                    self.save_model(f\"model_stockfish_pgn_game{game_count}.pt\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model(\"model_stockfish_pgn_final.pt\")\n",
    "        print(f\"PGN training completed! Processed {game_count} games and {processed_moves} moves.\")\n",
    "        \n",
    "        # Close Stockfish engine\n",
    "        if self.stockfish:\n",
    "            self.stockfish.quit()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        \"\"\"Save the model to disk\"\"\"\n",
    "        filepath = os.path.join(self.model_dir, filename)\n",
    "        torch.save(self.policy_net.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        \"\"\"Load a model from disk\"\"\"\n",
    "        filepath = os.path.join(self.model_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            # Load state dict, ensuring map_location handles CPU/GPU differences\n",
    "            self.policy_net.load_state_dict(torch.load(filepath, map_location=self.device))\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.policy_net.to(self.device)\n",
    "            self.target_net.to(self.device)\n",
    "            print(f\"Model loaded from {filepath} to {self.device}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Model file {filepath} not found\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data (Optional)\n",
    "\n",
    "If you don't have your own PGN data, you can generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_games=1000, output_file=\"/content/data/synthetic/synthetic_games.pgn\"):\n",
    "    \"\"\"Generate a dataset of random chess games\"\"\"\n",
    "    import chess\n",
    "    import chess.pgn\n",
    "    import random\n",
    "    import os\n",
    "    \n",
    "    def generate_random_game(max_moves=100):\n",
    "        \"\"\"Generate a random chess game\"\"\"\n",
    "        board = chess.Board()\n",
    "        game = chess.pgn.Game()\n",
    "        \n",
    "        # Set some game headers\n",
    "        game.headers[\"Event\"] = \"Synthetic Game\"\n",
    "        game.headers[\"Site\"] = \"Colab Synthetic Database\"\n",
    "        game.headers[\"Date\"] = \"2025.04.28\"\n",
    "        game.headers[\"Round\"] = \"1\"\n",
    "        game.headers[\"White\"] = \"Engine1\"\n",
    "        game.headers[\"Black\"] = \"Engine2\"\n",
    "        game.headers[\"Result\"] = \"*\"\n",
    "        \n",
    "        node = game\n",
    "        \n",
    "        # Make random moves until the game is over or max_moves is reached\n",
    "        move_count = 0\n",
    "        while not board.is_game_over() and move_count < max_moves:\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            move = random.choice(legal_moves)\n",
    "            board.push(move)\n",
    "            node = node.add_variation(move)\n",
    "            move_count += 1\n",
    "        \n",
    "        # Set the result\n",
    "        if board.is_checkmate():\n",
    "            result = \"1-0\" if board.turn == chess.BLACK else \"0-1\"\n",
    "        elif board.is_stalemate() or board.is_insufficient_material() or board.is_fifty_moves() or board.is_repetition():\n",
    "            result = \"1/2-1/2\"\n",
    "        else:\n",
    "            result = \"*\"\n",
    "        \n",
    "        game.headers[\"Result\"] = result\n",
    "        \n",
    "        return game\n",
    "\n",
    "    def save_pgn(game, filename):\n",
    "        \"\"\"Save a game to a PGN file\"\"\"\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, 'a') as f:\n",
    "            exporter = chess.pgn.FileExporter(f)\n",
    "            game.accept(exporter)\n",
    "            f.write(\"\\n\\n\")  # Add some space between games\n",
    "    \n",
    "    # Clear file if it exists\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        \n",
    "    for i in range(num_games):\n",
    "        game = generate_random_game()\n",
    "        save_pgn(game, output_file)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Generated {i + 1} games\")\n",
    "    \n",
    "    print(f\"Dataset generated and saved to {output_file}\")\n",
    "\n",
    "# Generate synthetic data if needed\n",
    "pgn_file = \"/content/data/synthetic/synthetic_games.pgn\"\n",
    "if not os.path.exists(pgn_file):\n",
    "    print(\"Generating synthetic data...\")\n",
    "    generate_synthetic_data(num_games=1000, output_file=pgn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Your Own PGN Data (Optional)\n",
    "\n",
    "If you have your own PGN data, you can upload it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to upload your own PGN file\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# pgn_file = next(iter(uploaded.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll train the model using both PGN data and self-play with Stockfish evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with Stockfish\n",
    "trainer = ChessTrainerWithStockfish()\n",
    "\n",
    "# Train from PGN data\n",
    "# If you uploaded your own PGN file, use that path instead\n",
    "pgn_losses = trainer.train_from_pgn(pgn_file, num_games=500)\n",
    "\n",
    "# Continue with self-play training\n",
    "self_play_rewards, self_play_lengths, self_play_losses = trainer.train_self_play(num_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Results\n",
    "\n",
    "Let's visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(rewards, lengths, losses):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(lengths)\n",
    "    plt.title('Episode Lengths')\n",
    "    plt.ylabel('Length')\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss (Optimization Steps)')\n",
    "    plt.xlabel('Optimization Step')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot self-play results\n",
    "plot_results(self_play_rewards, self_play_lengths, self_play_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the trained model to see how it plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Agent Implementation\n",
    "class ChessAgent:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.policy_net = DQN()\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.policy_net.load_state_dict(torch.load(model_path))\n",
    "            print(f\"Loaded model from {model_path}\")\n",
    "        self.policy_net.eval() # Set to evaluation mode by default\n",
    "\n",
    "    def select_move(self, board):\n",
    "        \"\"\"Selects the best move based on the current policy network.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = board_to_tensor(board)\n",
    "            move_mask = create_move_mask(board).unsqueeze(0)\n",
    "            \n",
    "            # Get Q-values for all moves\n",
    "            q_values = self.policy_net(state_tensor, move_mask)\n",
    "            \n",
    "            # Select the move with the highest Q-value\n",
    "            best_move_index = torch.argmax(q_values).item()\n",
    "            \n",
    "            # Map index back to move\n",
    "            from_square = best_move_index // 64\n",
    "            to_square = best_move_index % 64\n",
    "            \n",
    "            # Check if this move is actually legal (due to mask/promotion simplification)\n",
    "            potential_move = chess.Move(from_square, to_square)\n",
    "            # Handle promotion possibility\n",
    "            piece = board.piece_at(from_square)\n",
    "            if piece and piece.piece_type == chess.PAWN:\n",
    "                 if board.turn == chess.WHITE and chess.square_rank(to_square) == 7:\n",
    "                     potential_move.promotion = chess.QUEEN # Default to Queen promotion\n",
    "                 elif board.turn == chess.BLACK and chess.square_rank(to_square) == 0:\n",
    "                     potential_move.promotion = chess.QUEEN # Default to Queen promotion\n",
    "\n",
    "            if potential_move in board.legal_moves:\n",
    "                return potential_move\n",
    "            else:\n",
    "                # Fallback: choose a random legal move\n",
    "                legal_moves = list(board.legal_moves)\n",
    "                return random.choice(legal_moves) if legal_moves else None\n",
    "\n",
    "def test_agent(model_path=\"/content/models/model_stockfish_final.pt\"):\n",
    "    agent = ChessAgent(model_path=model_path)\n",
    "    \n",
    "    # Test the agent\n",
    "    board = chess.Board()\n",
    "    print(\"\\nTesting trained agent:\")\n",
    "    print(board.unicode())\n",
    "    \n",
    "    for i in range(10): # Play 10 moves\n",
    "        if board.is_game_over():\n",
    "            print(\"Game Over!\")\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nTurn: {'White' if board.turn == chess.WHITE else 'Black'}\")\n",
    "        \n",
    "        # Get move from agent\n",
    "        move = agent.select_move(board)\n",
    "        \n",
    "        print(f\"Agent selects move: {move.uci()}\")\n",
    "        board.push(move)\n",
    "        print(board.unicode())\n",
    "\n",
    "# Test the trained agent\n",
    "test_agent(\"/content/models/model_stockfish_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model for Download\n",
    "\n",
    "Finally, let's save the model for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model\n",
    "from google.colab import files\n",
    "files.download('/content/models/model_stockfish_final.pt')\n",
    "\n",
    "print(\"\\nTraining complete! You can now download the model file and use it with the chess GUI.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
