{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Bot Training on Google Colab\n",
    "\n",
    "This notebook allows you to train the deep reinforcement learning chess agent implemented in `drl_agent.py` using the logic from `train_model.py`.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Upload Files:** Upload the following files to your Colab environment (use the file browser on the left):\n",
    "    - `drl_agent.py` (contains the DQN model and agent logic)\n",
    "    - `synthetic_games.pgn` (the synthetic PGN data generated earlier, place it in a `data/synthetic` directory)\n",
    "    - *Alternatively, you can uncomment and run the data generation cell below if you don't have the PGN file.*\n",
    "2. **Install Dependencies:** Run the first code cell to install the necessary libraries.\n",
    "3. **Run Training:** Execute the subsequent cells to define the training components and start the training process.\n",
    "4. **Monitor Training:** Observe the output for training progress (loss, rewards, etc.).\n",
    "5. **Download Model:** Trained models will be saved periodically in the `/content/models` directory. Download the desired model file (e.g., `model_final.pt` or `model_pgn_final.pt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install python-chess==1.11.2 torch numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Generate Synthetic Data\n",
    "\n",
    "Run this cell only if you haven't uploaded `synthetic_games.pgn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chess
",
    "# import chess.pgn
",
    "# import random
",
    "# import io
",
    "# import os
",
    "# 
",
    "# def generate_random_game(max_moves=100):
",
    "#     \"\"\"Generate a random chess game\"\"\"
",
    "#     board = chess.Board()
",
    "#     game = chess.pgn.Game()
",
    "#     
",
    "#     # Set some game headers
",
    "#     game.headers[\"Event\"] = \"Synthetic Game\"
",
    "#     game.headers[\"Site\"] = \"Colab Synthetic Database\"
",
    "#     game.headers[\"Date\"] = \"2025.04.27\"
",
    "#     game.headers[\"Round\"] = \"1\"
",
    "#     game.headers[\"White\"] = \"Engine1\"
",
    "#     game.headers[\"Black\"] = \"Engine2\"
",
    "#     game.headers[\"Result\"] = \"*\"
",
    "#     
",
    "#     node = game
",
    "#     
",
    "#     # Make random moves until the game is over or max_moves is reached
",
    "#     move_count = 0
",
    "#     while not board.is_game_over() and move_count < max_moves:
",
    "#         legal_moves = list(board.legal_moves)
",
    "#         if not legal_moves:
",
    "#             break
",
    "#         
",
    "#         move = random.choice(legal_moves)
",
    "#         board.push(move)
",
    "#         node = node.add_variation(move)
",
    "#         move_count += 1
",
    "#     
",
    "#     # Set the result
",
    "#     if board.is_checkmate():
",
    "#         result = \"1-0\" if board.turn == chess.BLACK else \"0-1\"
",
    "#     elif board.is_stalemate() or board.is_insufficient_material() or board.is_fifty_moves() or board.is_repetition():
",
    "#         result = \"1/2-1/2\"
",
    "#     else:
",
    "#         result = \"*\"
",
    "#     
",
    "#     game.headers[\"Result\"] = result
",
    "#     
",
    "#     return game
",
    "# 
",
    "# def save_pgn(game, filename):
",
    "#     \"\"\"Save a game to a PGN file\"\"\"
",
    "#     # Ensure directory exists
",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)
",
    "#     with open(filename, 'a') as f:
",
    "#         exporter = chess.pgn.FileExporter(f)
",
    "#         game.accept(exporter)
",
    "#         f.write(\"\\n\\n\")  # Add some space between games
",
    "# 
",
    "# def generate_dataset(num_games=1000, output_file=\"data/synthetic/synthetic_games.pgn\"):
",
    "#     \"\"\"Generate a dataset of random chess games\"\"\"
",
    "#     # Clear file if it exists
",
    "#     if os.path.exists(output_file):
",
    "#         os.remove(output_file)
",
    "#         
",
    "#     for i in range(num_games):
",
    "#         game = generate_random_game()
",
    "#         save_pgn(game, output_file)
",
    "#         if (i + 1) % 100 == 0:
",
    "#             print(f\"Generated {i + 1} games\")
",
    "# 
",
    "# # Generate 1000 random games
",
    "# print(\"Generating synthetic chess games...\")
",
    "# generate_dataset(num_games=1000)
",
    "# print(f\"Dataset generated and saved to data/synthetic/synthetic_games.pgn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Define Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess
",
    "import chess.pgn
",
    "import torch
",
    "import torch.nn as nn
",
    "import torch.optim as optim
",
    "import numpy as np
",
    "import random
",
    "import os
",
    "import io
",
    "from collections import namedtuple, deque
",
    "import time
",
    "\n",
    "# Import from uploaded drl_agent.py
",
    "from drl_agent import DQN, ChessAgent, board_to_tensor, create_move_mask
",
    "\n",
    "# Define the Experience Replay memory
",
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done', 'mask', 'next_mask'))
",
    "\n",
    "class ReplayMemory:
",
    "    def __init__(self, capacity):
",
    "        self.memory = deque([], maxlen=capacity)
",
    "        
",
    "    def push(self, *args):
",
    "        \"\"\"Save an experience\"\"\"
",
    "        self.memory.append(Experience(*args))
",
    "        
",
    "    def sample(self, batch_size):
",
    "        \"\"\"Sample a batch of experiences\"\"\"
",
    "        return random.sample(self.memory, batch_size)
",
    "    
",
    "    def __len__(self):
",
    "        return len(self.memory)
",
    "\n",
    "class ChessTrainer:
",
    "    def __init__(self, model_dir=\"/content/models\"):
",
    "        # Create model directory if it doesn't exist
",
    "        if not os.path.exists(model_dir):
",
    "            os.makedirs(model_dir)
",
    "        
",
    "        self.model_dir = model_dir
",
    "        
",
    "        # Check for GPU availability
",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
",
    "        print(f\"Using device: {self.device}\")
",
    "        
",
    "        # Initialize networks
",
    "        self.policy_net = DQN().to(self.device)
",
    "        self.target_net = DQN().to(self.device)
",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())
",
    "        self.target_net.eval()  # Target network is only used for inference
",
    "        
",
    "        # Initialize optimizer
",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)
",
    "        
",
    "        # Initialize replay memory
",
    "        self.memory = ReplayMemory(10000) # Adjust capacity as needed
",
    "        
",
    "        # Training parameters
",
    "        self.batch_size = 128 # Increase batch size for GPU
",
    "        self.gamma = 0.99  # Discount factor
",
    "        self.eps_start = 0.9
",
    "        self.eps_end = 0.05
",
    "        self.eps_decay = 20000 # Adjust decay rate
",
    "        self.target_update = 20  # Update target network every N episodes/batches
",
    "        
",
    "        self.steps_done = 0
",
    "        
",
    "    def select_action(self, state, mask, board):
",
    "        \"\"\"Select an action using epsilon-greedy policy\"\"\"
",
    "        sample = random.random()
",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \
",
    "                        np.exp(-1. * self.steps_done / self.eps_decay)
",
    "        self.steps_done += 1
",
    "        
",
    "        state = state.to(self.device)
",
    "        mask = mask.to(self.device)
",
    "        
",
    "        if sample > eps_threshold:
",
    "            with torch.no_grad():
",
    "                # Use policy network to select best action
",
    "                q_values = self.policy_net(state, mask)
",
    "                action_idx = q_values.max(1)[1].item()
",
    "                
",
    "                # Convert action index to chess move
",
    "                from_square = action_idx // 64
",
    "                to_square = action_idx % 64
",
    "                
",
    "                # Check if this is a legal move
",
    "                move = chess.Move(from_square, to_square)
",
    "                
",
    "                # Handle promotion
",
    "                piece = board.piece_at(from_square)
",
    "                if piece and piece.piece_type == chess.PAWN:
",
    "                    if board.turn == chess.WHITE and chess.square_rank(to_square) == 7:
",
    "                        move.promotion = chess.QUEEN
",
    "                    elif board.turn == chess.BLACK and chess.square_rank(to_square) == 0:
",
    "                        move.promotion = chess.QUEEN
",
    "                
",
    "                # If move is not legal, choose a random legal move
",
    "                if move not in board.legal_moves:
",
    "                    # print(f\"Warning: Predicted move {move.uci()} not legal. Choosing random.\") # Debug
",
    "                    legal_moves = list(board.legal_moves)
",
    "                    if not legal_moves: return None, None # No legal moves
",
    "                    move = random.choice(legal_moves)
",
    "                    action_idx = move.from_square * 64 + move.to_square
",
    "        else:
",
    "            # Choose a random legal move
",
    "            legal_moves = list(board.legal_moves)
",
    "            if not legal_moves: return None, None # No legal moves
",
    "            move = random.choice(legal_moves)
",
    "            action_idx = move.from_square * 64 + move.to_square
",
    "            
",
    "        return action_idx, move
",
    "    
",
    "    def optimize_model(self):
",
    "        \"\"\"Perform one step of optimization\"\"\"
",
    "        if len(self.memory) < self.batch_size:
",
    "            return None
",
    "        
",
    "        # Sample a batch from memory
",
    "        experiences = self.memory.sample(self.batch_size)
",
    "        batch = Experience(*zip(*experiences))
",
    "        
",
    "        # Convert to tensors and move to device
",
    "        state_batch = torch.cat(batch.state).to(self.device)
",
    "        action_batch = torch.tensor(batch.action, dtype=torch.long, device=self.device).unsqueeze(1)
",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=self.device)
",
    "        mask_batch = torch.cat(batch.mask).to(self.device)
",
    "        
",
    "        # Handle non-final states
",
    "        non_final_mask_indices = torch.tensor(tuple(map(lambda d: not d, batch.done)), dtype=torch.bool, device=self.device)
",
    "        
",
    "        non_final_next_states = torch.cat([s for s, d in zip(batch.next_state, batch.done) if not d]).to(self.device)
",
    "        non_final_next_masks = torch.cat([m for m, d in zip(batch.next_mask, batch.done) if not d]).to(self.device)
",
    "        
",
    "        # Compute Q(s_t, a)
",
    "        # The model computes Q(s_t), then we select the columns of actions taken.
",
    "        state_action_values = self.policy_net(state_batch, mask_batch).gather(1, action_batch)
",
    "        
",
    "        # Compute V(s_{t+1}) for all next states.
",
    "        # Expected values of actions for non_final_next_states are computed based
",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].
",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)
",
    "        if non_final_next_states.size(0) > 0: # Check if there are any non-final states
",
    "             with torch.no_grad():
",
    "                 next_state_values[non_final_mask_indices] = self.target_net(non_final_next_states, non_final_next_masks).max(1)[0]
",
    "        
",
    "        # Compute the expected Q values: Q_expected = r + gamma * max_a' Q_target(s', a')
",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch
",
    "        
",
    "        # Compute Huber loss (Smooth L1 Loss)
",
    "        criterion = nn.SmoothL1Loss()
",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
",
    "        
",
    "        # Optimize the model
",
    "        self.optimizer.zero_grad()
",
    "        loss.backward()
",
    "        # Clip gradients to stabilize training
",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 1.0) # Use clip_grad_value_
",
    "        self.optimizer.step()
",
    "        
",
    "        return loss.item()
",
    "    
",
    "    def train_self_play(self, num_episodes=1000):
",
    "        \"\"\"Train the model through self-play\"\"\"
",
    "        print(f\"Starting self-play training for {num_episodes} episodes...\")
",
    "        
",
    "        episode_rewards = []
",
    "        episode_lengths = []
",
    "        losses = []
",
    "        start_time = time.time()
",
    "        
",
    "        for episode in range(num_episodes):
",
    "            # Initialize the environment
",
    "            board = chess.Board()
",
    "            episode_reward = 0
",
    "            episode_length = 0
",
    "            
",
    "            # Get initial state
",
    "            state = board_to_tensor(board)
",
    "            mask = create_move_mask(board)
",
    "            
",
    "            done = False
",
    "            while not done:
",
    "                # Select and perform an action
",
    "                action_idx, move = self.select_action(state, mask, board)
",
    "                
",
    "                if move is None: # No legal moves
",
    "                    break 
",
    "                    
",
    "                # Execute the move
",
    "                board.push(move)
",
    "                
",
    "                # Get the next state
",
    "                next_state = board_to_tensor(board)
",
    "                next_mask = create_move_mask(board)
",
    "                
",
    "                # Calculate reward
",
    "                reward = self.calculate_reward(board)
",
    "                
",
    "                # Check if the game is over
",
    "                done = board.is_game_over()
",
    "                
",
    "                # Store the transition in memory
",
    "                self.memory.push(state, action_idx, next_state, reward, done, mask, next_mask)
",
    "                
",
    "                # Move to the next state
",
    "                state = next_state
",
    "                mask = next_mask
",
    "                
",
    "                # Perform one step of optimization
",
    "                loss = self.optimize_model()
",
    "                if loss is not None:
",
    "                    losses.append(loss)
",
    "                
",
    "                episode_reward += reward
",
    "                episode_length += 1
",
    "                
",
    "                # Limit episode length to avoid very long games
",
    "                if episode_length >= 200:
",
    "                    done = True
",
    "            
",
    "            # Update the target network
",
    "            if episode % self.target_update == 0:
",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())
",
    "            
",
    "            episode_rewards.append(episode_reward)
",
    "            episode_lengths.append(episode_length)
",
    "            
",
    "            # Print progress
",
    "            if (episode + 1) % 10 == 0:
",
    "                avg_reward = np.mean(episode_rewards[-10:])
",
    "                avg_length = np.mean(episode_lengths[-10:])
",
    "                avg_loss = np.mean(losses[-100:]) if losses else 0 # Avg loss over last 100 steps
",
    "                elapsed_time = time.time() - start_time
",
    "                print(f\"Episode {episode+1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Avg Length: {avg_length:.1f} | Avg Loss: {avg_loss:.4f} | Steps: {self.steps_done} | Time: {elapsed_time:.1f}s\")
",
    "            
",
    "            # Save model periodically
",
    "            if (episode + 1) % 100 == 0:
",
    "                self.save_model(f\"model_selfplay_ep{episode+1}.pt\")
",
    "        
",
    "        # Save final model
",
    "        self.save_model(\"model_selfplay_final.pt\")
",
    "        print(\"Self-play training completed!\")
",
    "        
",
    "        return episode_rewards, episode_lengths, losses
",
    "    
",
    "    def calculate_reward(self, board):
",
    "        \"\"\"Calculate reward based on the board state\"\"\"
",
    "        # Basic reward function
",
    "        if board.is_checkmate():
",
    "            # High reward/penalty for checkmate (from perspective of player whose turn it is)
",
    "            # If it's White's turn, Black just checkmated White -> reward is -1
",
    "            # If it's Black's turn, White just checkmated Black -> reward is -1
",
    "            return -1.0 
",
    "        elif board.is_stalemate() or board.is_insufficient_material() or board.is_seventyfive_moves() or board.is_fivefold_repetition():
",
    "            # Neutral reward for draw
",
    "            return 0.0
",
    "        
",
    "        # Intermediate reward (e.g., based on material difference)
",
    "        # Calculate material difference from the perspective of the player who just moved
",
    "        turn = not board.turn # Get the player who just moved
",
    "        material_diff = self.calculate_material_advantage(board, turn)
",
    "        
",
    "        # Small reward for material advantage, scaled
",
    "        return 0.01 * material_diff
",
    "        
",
    "    def calculate_material_advantage(self, board, turn):
",
    "        \"\"\"Calculate material advantage for the specified player (turn=True for White, False for Black)\"\"\"
",
    "        piece_values = {
",
    "            chess.PAWN: 1,
",
    "            chess.KNIGHT: 3,
",
    "            chess.BISHOP: 3,
",
    "            chess.ROOK: 5,
",
    "            chess.QUEEN: 9,
",
    "            chess.KING: 0
",
    "        }
",
    "        
",
    "        white_material = sum(len(board.pieces(piece_type, chess.WHITE)) * value 
",
    "                            for piece_type, value in piece_values.items())
",
    "        black_material = sum(len(board.pieces(piece_type, chess.BLACK)) * value 
",
    "                            for piece_type, value in piece_values.items())
",
    "        
",
    "        # Return advantage from perspective of the specified player
",
    "        return white_material - black_material if turn == chess.WHITE else black_material - white_material
",
    "    
",
    "    # evaluate_position function can be added here if needed for reward shaping
",
    "    
",
    "    def save_model(self, filename):
",
    "        \"\"\"Save the model to disk\"\"\"
",
    "        filepath = os.path.join(self.model_dir, filename)
",
    "        torch.save(self.policy_net.state_dict(), filepath)
",
    "        print(f\"Model saved to {filepath}\")
",
    "    
",
    "    def load_model(self, filename):
",
    "        \"\"\"Load a model from disk\"\"\"
",
    "        filepath = os.path.join(self.model_dir, filename)
",
    "        if os.path.exists(filepath):
",
    "            # Load state dict, ensuring map_location handles CPU/GPU differences
",
    "            self.policy_net.load_state_dict(torch.load(filepath, map_location=self.device))
",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())
",
    "            self.policy_net.to(self.device)
",
    "            self.target_net.to(self.device)
",
    "            print(f\"Model loaded from {filepath} to {self.device}\")
",
    "            return True
",
    "        else:
",
    "            print(f\"Model file {filepath} not found\")
",
    "            return False
",
    "    
",
    "    def train_from_pgn(self, pgn_file, num_games=1000):
",
    "        \"\"\"Train the model from PGN games (Supervised Learning + RL Reward)\"\"\"
",
    "        print(f\"Starting training from PGN file: {pgn_file}\")
",
    "        
",
    "        if not os.path.exists(pgn_file):
",
    "            print(f\"Error: PGN file not found at {pgn_file}\")
",
    "            return []
",
    "            
",
    "        # Open PGN file
",
    "        with open(pgn_file) as f:
",
    "            game_count = 0
",
    "            losses = []
",
    "            processed_moves = 0
",
    "            start_time = time.time()
",
    "            
",
    "            while game_count < num_games:
",
    "                # Read the next game
",
    "                try:
",
    "                    game = chess.pgn.read_game(f)
",
    "                except Exception as e:
",
    "                    print(f\"Error reading game: {e}. Skipping.\")
",
    "                    continue
",
    "                    
",
    "                if game is None:
",
    "                    print(\"Reached end of PGN file.\")
",
    "                    break  # End of file
",
    "                
",
    "                # Process the game
",
    "                board = game.board()
",
    "                moves = list(game.mainline_moves())
",
    "                
",
    "                # Skip very short games
",
    "                if len(moves) < 5:
",
    "                    continue
",
    "                
",
    "                # Process each position in the game
",
    "                for i, move in enumerate(moves):
",
    "                    # Get current state
",
    "                    state = board_to_tensor(board)
",
    "                    mask = create_move_mask(board)
",
    "                    
",
    "                    # Convert actual move to action index
",
    "                    action_idx = move.from_square * 64 + move.to_square
",
    "                    
",
    "                    # Make the move on a copy to get next state
",
    "                    next_board = board.copy()
",
    "                    next_board.push(move)
",
    "                    
",
    "                    # Get next state
",
    "                    next_state = board_to_tensor(next_board)
",
    "                    next_mask = create_move_mask(next_board)
",
    "                    
",
    "                    # Calculate reward for the state *after* the move
",
    "                    reward = self.calculate_reward(next_board)
",
    "                    
",
    "                    # Check if game is over after the move
",
    "                    done = next_board.is_game_over()
",
    "                    
",
    "                    # Store transition in memory
",
    "                    self.memory.push(state, action_idx, next_state, reward, done, mask, next_mask)
",
    "                    
",
    "                    # Make the move on the main board for the next iteration
",
    "                    board.push(move)
",
    "                    processed_moves += 1
",
    "                    
",
    "                    # Perform optimization step if enough samples
",
    "                    if len(self.memory) >= self.batch_size:
",
    "                        loss = self.optimize_model()
",
    "                        if loss is not None:
",
    "                            losses.append(loss)
",
    "                            
",
    "                        # Update target network periodically based on steps/batches
",
    "                        if processed_moves % (self.target_update * self.batch_size) == 0: 
",
    "                            self.target_net.load_state_dict(self.policy_net.state_dict())
",
    "            
",
    "                game_count += 1
",
    "                
",
    "                # Print progress
",
    "                if game_count % 10 == 0:
",
    "                    avg_loss = np.mean(losses[-1000:]) if losses else 0 # Avg loss over last 1000 steps
",
    "                    elapsed_time = time.time() - start_time
",
    "                    print(f\"Processed {game_count}/{num_games} games | Moves: {processed_moves} | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")
",
    "                
",
    "                # Save model periodically
",
    "                if game_count % 100 == 0:
",
    "                    self.save_model(f\"model_pgn_game{game_count}.pt\")
",
    "        
",
    "        # Save final model
",
    "        self.save_model(\"model_pgn_final.pt\")
",
    "        print(f\"PGN training completed! Processed {game_count} games and {processed_moves} moves.\")
",
    "        
",
    "        return losses"
",
    "    
",
    "# Instantiate trainer
",
    "trainer = ChessTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Training\n",
    "\n",
    "You can choose to train from the PGN file first, then continue with self-play, or just do one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train from PGN data first
",
    "pgn_file = \"/content/data/synthetic/synthetic_games.pgn\" # Make sure this path is correct
",
    "pgn_losses = trainer.train_from_pgn(pgn_file, num_games=500) # Adjust num_games as needed
",
    "\n",
    "# Option 2: Start or continue with self-play training
",
    "# trainer.load_model(\"model_pgn_final.pt\") # Load PGN-trained model if desired
",
    "self_play_rewards, self_play_lengths, self_play_losses = trainer.train_self_play(num_episodes=500) # Adjust num_episodes as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Optional) Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt
",
    "\n",
    "def plot_results(rewards, lengths, losses):
",
    "    plt.figure(figsize=(12, 8))
",
    "    
",
    "    plt.subplot(3, 1, 1)
",
    "    plt.plot(rewards)
",
    "    plt.title('Episode Rewards')
",
    "    plt.ylabel('Reward')
",
    "    
",
    "    plt.subplot(3, 1, 2)
",
    "    plt.plot(lengths)
",
    "    plt.title('Episode Lengths')
",
    "    plt.ylabel('Length')
",
    "    
",
    "    plt.subplot(3, 1, 3)
",
    "    plt.plot(losses)
",
    "    plt.title('Training Loss (Optimization Steps)')
",
    "    plt.xlabel('Optimization Step')
",
    "    plt.ylabel('Loss')
",
    "    
",
    "    plt.tight_layout()
",
    "    plt.show()
",
    "\n",
    "# Plot self-play results if available
",
    "if 'self_play_rewards' in locals():
",
    "    plot_results(self_play_rewards, self_play_lengths, self_play_losses)
",
    "    
",
    "# You can also plot PGN losses if needed
",
    "# if 'pgn_losses' in locals():
",
    "#     plt.figure(figsize=(12, 4))
",
    "#     plt.plot(pgn_losses)
",
    "#     plt.title('PGN Training Loss')
",
    "#     plt.xlabel('Optimization Step')
",
    "#     plt.ylabel('Loss')
",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final trained model
",
    "agent = ChessAgent() # Instantiate agent
",
    "agent.policy_net.to(trainer.device) # Ensure agent uses the correct device
",
    "loaded = trainer.load_model(\"model_selfplay_final.pt\") # Or model_pgn_final.pt
",
    "\n",
    "if loaded:
",
    "    agent.policy_net.load_state_dict(trainer.policy_net.state_dict()) # Copy loaded state to agent
",
    "    agent.policy_net.eval() # Set agent's model to evaluation mode
",
    "    
",
    "    # Test the agent
",
    "    board = chess.Board()
",
    "    print(\"\nTesting trained agent:\")
",
    "    print(board.unicode())
",
    "    
",
    "    for i in range(10): # Play 10 moves
",
    "        if board.is_game_over():
",
    "            print(\"Game Over!\")
",
    "            break
",
    "            
",
    "        print(f\"\nTurn: {'White' if board.turn == chess.WHITE else 'Black'}\")
",
    "        
",
    "        # Get move from agent
",
    "        state_tensor = board_to_tensor(board).to(trainer.device)
",
    "        mask_tensor = create_move_mask(board).to(trainer.device)
",
    "        
",
    "        # Use the agent's select_move logic (which includes epsilon-greedy, but we want deterministic here)
",
    "        # Let's directly use the network output for testing
",
    "        with torch.no_grad():
",
    "            q_values = agent.policy_net(state_tensor, mask_tensor)
",
    "            action_idx = q_values.max(1)[1].item()
",
    "            
",
    "            from_square = action_idx // 64
",
    "            to_square = action_idx % 64
",
    "            move = chess.Move(from_square, to_square)
",
    "            
",
    "            # Handle promotion
",
    "            piece = board.piece_at(from_square)
",
    "            if piece and piece.piece_type == chess.PAWN:
",
    "                if board.turn == chess.WHITE and chess.square_rank(to_square) == 7:
",
    "                    move.promotion = chess.QUEEN
",
    "                elif board.turn == chess.BLACK and chess.square_rank(to_square) == 0:
",
    "                    move.promotion = chess.QUEEN
",
    "            
",
    "            # Fallback if predicted move is illegal
",
    "            if move not in board.legal_moves:
",
    "                print(f\"Warning: Predicted move {move.uci()} not legal. Choosing random.\")
",
    "                legal_moves = list(board.legal_moves)
",
    "                if not legal_moves: break
",
    "                move = random.choice(legal_moves)
",
    "        
",
    "        print(f\"Agent selects move: {move.uci()}\")
",
    "        board.push(move)
",
    "        print(board.unicode())
",
    "else:
",
    "    print(\"Could not load trained model for testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

